{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc2d9bec-6e94-4547-8de6-2ff4c0b96220",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df01830c-7f53-47ce-89ce-c93a03c679aa",
   "metadata": {},
   "source": [
    "### Question-Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c3c9ae2-28d4-492e-9b4e-a0263c6b702d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForQuestionAnswering.\n",
      "\n",
      "All the layers of TFBertForQuestionAnswering were initialized from the model checkpoint at pierreguillou/bert-base-cased-squad-v1.1-portuguese.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForQuestionAnswering for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"question-answering\",model=\"pierreguillou/bert-base-cased-squad-v1.1-portuguese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4e11569-4b03-4458-8197-2b0d58140a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qea(pipe, question):\n",
    "    resposta = pipe(question=question,context=\"Carl Sagan foi um cientista norte-americano. Sagan é autor de mais de 600 publicações científicas e também de mais de vinte livros de ciência e ficcção\" )\n",
    "    return print(f\"{resposta['answer']}\\nScore: {resposta['score']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90d34503-6843-429d-a19a-12af45182a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mais de vinte\n",
      "Score: 0.42243295907974243\n",
      "um cientista norte-americano\n",
      "Score: 0.20211566984653473\n",
      "mais de 600\n",
      "Score: 0.641075611114502\n",
      "ciência e ficcção\n",
      "Score: 0.6063461303710938\n"
     ]
    }
   ],
   "source": [
    "qea(pipe, \"Quantos livros Carl Sagan tem publicado?\")\n",
    "qea(pipe, \"quem é carl sagan?\")\n",
    "qea(pipe, \"quantos publicações cientificas tem carl sagan?\")\n",
    "qea(pipe, \"quais temas carl sagan escreve sobre?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a14b36-0f25-4d3e-bd16-a9b304d1c17c",
   "metadata": {},
   "source": [
    "### Fill-mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f603751-31cc-4547-b259-c8827651f592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d10f1aebbdaa4bc39804febfc70ce081",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/647 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\znaya\\anaconda1\\lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\znaya\\.cache\\huggingface\\hub\\models--neuralmind--bert-base-portuguese-cased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6963a6293e9a4b6d9f837de69856ad01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tf_model.h5:   0%|          | 0.00/529M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the layers of TFBertForMaskedLM were initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e4ce6249ddf42a9991fb513b2d5bcc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/43.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96fd7639bc394f5a82c825f1dbb06c3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/210k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b09f63b8b6643828914536783a58d76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0644ff734124510bba4496267c44434",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mask = pipeline(\"fill-mask\",model=\"neuralmind/bert-base-portuguese-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa51603d-6853-4abd-8341-5d48cb9b854c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_(mask, text):\n",
    "\ttext = text + ' [MASK]'\n",
    "\tresposta = mask(text)\n",
    "\tfor r in range(len(resposta)):\n",
    "\t\tprint(resposta[r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f9f4272-b99e-4d42-881e-3e65e3eaf5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.5944021344184875, 'token': 8105, 'token_str': 'chão', 'sequence': 'Existe uma chance do copo cair no chão'}\n",
      "{'score': 0.045443687587976456, 'token': 2187, 'token_str': 'rio', 'sequence': 'Existe uma chance do copo cair no rio'}\n",
      "{'score': 0.04219283163547516, 'token': 528, 'token_str': 'mar', 'sequence': 'Existe uma chance do copo cair no mar'}\n",
      "{'score': 0.033419929444789886, 'token': 4848, 'token_str': 'fogo', 'sequence': 'Existe uma chance do copo cair no fogo'}\n",
      "{'score': 0.02256271243095398, 'token': 14575, 'token_str': 'lixo', 'sequence': 'Existe uma chance do copo cair no lixo'}\n"
     ]
    }
   ],
   "source": [
    "mask_(mask, \"Existe uma chance do copo cair no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ff75ac4-4f51-40b1-babe-8c78e5b3916e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.5338905453681946, 'token': 5236, 'token_str': 'acidente', 'sequence': 'O carro sofreu um acidente'}\n",
      "{'score': 0.12154246866703033, 'token': 119, 'token_str': '.', 'sequence': 'O carro sofreu um.'}\n",
      "{'score': 0.06548644602298737, 'token': 8802, 'token_str': 'incêndio', 'sequence': 'O carro sofreu um incêndio'}\n",
      "{'score': 0.033885229378938675, 'token': 100, 'token_str': '[UNK]', 'sequence': 'O carro sofreu um'}\n",
      "{'score': 0.03154251351952553, 'token': 6814, 'token_str': 'grave', 'sequence': 'O carro sofreu um grave'}\n"
     ]
    }
   ],
   "source": [
    "mask_(mask, \"O carro sofreu um\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07b0482-f240-47b1-9db2-a399af97dcf6",
   "metadata": {},
   "source": [
    "### Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "48945d6f-6541-4b55-a9b2-5b6ef1e25113",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to google-t5/t5-small and revision d769bba (https://huggingface.co/google-t5/t5-small).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "C:\\Users\\znaya\\anaconda1\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f1a18e47df345d589ec481c77d321a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\znaya\\anaconda1\\lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\znaya\\.cache\\huggingface\\hub\\models--google-t5--t5-small. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caddcdb51ec5483faf25caeb40c22eca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "584a252a993b4d658bce98a6121742f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8471c92ab83d47a98c69a91b23867037",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e68126b7a294479aa242a3825426ce8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summ = pipeline(\"summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "237e27cb-99a0-4da0-b799-7b905a9c93f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sumar(summ, text):\n",
    "    resposta = summ(text)\n",
    "    print(resposta[0]['summary_text'])\n",
    "\t "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6ea6de7c-9883-4a5d-8b80-76122af9543e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a Segunda Guerra Mundial foi um conflicto militar global que durou de 1939 a 1945, envolvendo a maioria das naçes do mundo, com com mais de 100 milhe de militares mobilizados . a guerra total, os principais dedicaram toda sua capacidade econômica, industrial e cientfica a servi\n"
     ]
    }
   ],
   "source": [
    "sumar(summ,\"\"\"A Segunda Guerra Mundial foi um conflito militar global que durou de 1939 a 1945, envolvendo a maioria das nações do mundo — incluindo todas as grandes potências — organizadas em duas alianças militares opostas: os Aliados e o Eixo. Foi a guerra mais abrangente da história, com mais de 100 milhões de militares mobilizados. Em estado de 'guerra total', os principais envolvidos dedicaram toda sua capacidade econômica, industrial e científica a serviço dos esforços de guerra, deixando de lado a distinção entre recursos civis e militares. Marcado por um número significante de ataques contra civis, incluindo o Holocausto e a única vez em que armas nucleares foram utilizadas em combate, foi o conflito mais letal da história da humanidade, resultando entre 50 a mais de 70 milhões de mortes.[1]\n",
    "              Geralmente considera-se o ponto inicial da guerra como sendo a invasão da Polônia pela Alemanha Nazista em 1 de setembro de 1939 e subsequentes declarações de guerra contra a Alemanha pela França e pela maioria dos países do Império Britânico e da Commonwealth. Alguns países já estavam em guerra nesta época, como Etiópia e Reino de Itália na Segunda Guerra Ítalo-Etíope e China e Japão na Segunda Guerra Sino-Japonesa.[2] Muitos dos que não se envolveram inicialmente acabaram aderindo ao conflito em resposta a eventos como a invasão da União Soviética pelos alemães e os ataques japoneses contra as forças dos Estados Unidos no Pacífico em Pearl Harbor e em colônias ultra marítimas britânicas, que resultaram em declarações de guerra contra o Japão pelos Estados Unidos, Países Baixos e o Commonwealth Britânico.[3][4]\n",
    "              A guerra terminou com a vitória dos Aliados em 1945, alterando significativamente o alinhamento político e a estrutura social mundial. Enquanto a Organização das Nações Unidas (ONU) era estabelecida para estimular a cooperação global e evitar futuros conflitos, a União Soviética e os Estados Unidos emergiam como superpotências rivais, preparando o terreno para uma Guerra Fria que se estenderia pelos quarenta e seis anos seguintes (1945–1991). Nesse ínterim, a aceitação do princípio de autodeterminação acelerou movimentos de descolonização na Ásia e na África, enquanto a Europa ocidental dava início a um movimento de recuperação econômica e integração política.\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c2d922-5405-46bc-ba29-f7f60801f7c2",
   "metadata": {},
   "source": [
    "### Text Gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2860ccdc-a719-40c3-8a4f-113188a70efa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b50ac37cba8d4b37947247f9e0bbe47f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/666 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\znaya\\anaconda1\\lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\znaya\\.cache\\huggingface\\hub\\models--pierreguillou--gpt2-small-portuguese. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aafc500fcaa04974bc693edb41b22ede",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tf_model.h5:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at pierreguillou/gpt2-small-portuguese.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fe7cda612634fe0a3fe3df0874e43ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/92.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b4e311f9aea464990dc72a8e8eb4b88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/850k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffb078b395b64973a06dbeeb9e9fce6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/508k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2a12b150f404e6d9d199516c4444873",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/120 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text_gen = pipeline(\"text-generation\", model=\"pierreguillou/gpt2-small-portuguese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bccb3c86-f997-4c79-af5b-24a09d6db2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(text_gen, text):\n",
    "    answer = text_gen(text, max_length=60, do_sample=True)\n",
    "    print(answer[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7d4a4599-861b-4894-8d06-20d327b19f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto de bom dia é normalmente usada para descrever o comportamento humano e para identificar a fonte de dano, a causa do dano, o tempo que foi necessário para reparar e a natureza do dano que a causa causou. Neste tipo de diagnóstico, é possível calcular o risco de ocorrência de doenças graves, como\n",
      "Texto de feliz aniversario.\n",
      "\n",
      "Nos séculos anteriores a colonização europeia foi duramente atacada por piratas do norte. A guerra entre piratas continuou no Mediterrâneo oriental e no Mediterrâneo ocidental. Já o último grande conflito começou a ser combatido pelo Sultanato de Rum moderno (também chamado \"Império Bizantino\" em\n",
      "Texto para email de agradecimento ao chefe de estado e presidente da república. No mesmo dia, após ser acusado da fraude, foi liberado de todos os cargos públicos, exceto a magistratura.\n",
      "\n",
      "A investigação foi iniciada na manhã de 26 de outubro. O presidente da República, Michel Temer, admitiu\n"
     ]
    }
   ],
   "source": [
    "gen(text_gen,\"Texto de bom dia\")\n",
    "gen(text_gen,\"Texto de feliz aniversario\")\n",
    "gen(text_gen,\"Texto para email de agradecimento ao chefe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1399029-922c-40c9-973e-3eb80abb1a4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
