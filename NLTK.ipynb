{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8a11847-cf16-4706-8039-1b5e0ef20f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\znaya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\znaya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     C:\\Users\\znaya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\znaya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\znaya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\znaya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\znaya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer, WordNetLemmatizer,\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.tag import pos_tag, pos_tag_sents\n",
    "import string\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"tagsets\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "nltk.download(\"maxent_ne_chunker\")\n",
    "nltk.download(\"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b9445a-e8b2-4006-8507-0739ad8d40d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9feceb2-3677-4ae6-9c3d-4f19bab287dd",
   "metadata": {},
   "source": [
    "### Tokenização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e38d766-07ee-4b88-a519-1b7a7c06a0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = \"Não espere pelo momento perfeito. Faça de cada momento a oportunidade perfeita. Não desista, você consegue\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9885808-c42b-4e43-8a09-600bee41b9ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "['Não espere pelo momento perfeito.', 'Faça de cada momento a oportunidade perfeita.', 'Não desista, você consegue']\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "sentencas = sent_tokenize(texto, language=\"portuguese\")\n",
    "print(type(sentencas))\n",
    "print(sentencas)\n",
    "print(len(sentencas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8e0bc03-2489-4a67-bb35-d748a5639d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "['Não', 'espere', 'pelo', 'momento', 'perfeito', '.', 'Faça', 'de', 'cada', 'momento', 'a', 'oportunidade', 'perfeita', '.', 'Não', 'desista', ',', 'você', 'consegue']\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "word_tokens = word_tokenize(texto, language='portuguese')\n",
    "print(type(word_tokens))\n",
    "print(word_tokens)\n",
    "print(len(word_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e6e983-29f9-413a-a323-1de21110d038",
   "metadata": {},
   "source": [
    "### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a105560-b693-4f83-a166-2175ebae9267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207\n",
      "['a', 'à', 'ao', 'aos', 'aquela', 'aquelas', 'aquele', 'aqueles', 'aquilo', 'as']\n"
     ]
    }
   ],
   "source": [
    "stops = stopwords.words(\"portuguese\")\n",
    "print(len(stops))\n",
    "print([stops[i] for i in range(0,10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ae0f866d-95c8-4f00-a131-ce20b7c6ad39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Não espere momento perfeito . Faça cada momento oportunidade perfeita . Não desista , consegue 94\n",
      "Não espere pelo momento perfeito. Faça de cada momento a oportunidade perfeita. Não desista, você consegue 106\n"
     ]
    }
   ],
   "source": [
    "word_sem_stops = [t for t in word_tokens if t not in stops]\n",
    "texto_sem_stops = ' '.join(word_sem_stops)\n",
    "print(texto_sem_stops, len(texto_sem_stops))\n",
    "print(texto, len(texto))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d7a8b1f1-3992-440a-8a9c-45537d3f177c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Não espere momento perfeito  Faça cada momento oportunidade perfeita  Não desista  consegue 91\n"
     ]
    }
   ],
   "source": [
    "pontuacao = string.punctuation\n",
    "word_sem_stops_pontuacao = [t for t in texto_sem_stops if t not in pontuacao]\n",
    "word_sem_stops_pontuacao = ''.join(word_sem_stops_pontuacao)\n",
    "print(word_sem_stops_pontuacao, len(word_sem_stops_pontuacao))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e0382f-bb9b-4c97-acf2-a84a802c1cb9",
   "metadata": {},
   "source": [
    "### Distribuição de frequência de tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "67a5e766-e4a5-4152-aee5-cb9e148d644f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nltk.probability.FreqDist'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FreqDist({'Não': 2, 'momento': 2, '.': 2, 'espere': 1, 'pelo': 1, 'perfeito': 1, 'Faça': 1, 'de': 1, 'cada': 1, 'a': 1, ...})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist = nltk.FreqDist(word_tokens)\n",
    "print(type(dist))\n",
    "dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6f527666-f6eb-4114-b339-bd20f6a28c18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Não', 2), ('momento', 2)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist.most_common(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f563c5-e6b5-4d53-8bef-a34c56eb7089",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1e88a420-979d-4bd2-8615-4a5a90fbc285",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_string(list_):\n",
    "    return ' '.join(list_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7346b1f9-9e65-45d1-bfa1-8b91b50341d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Não esper pelo momento perfeito . Faça de cada momento a oportunidad perfeita . Não desista , você consegu\n",
      "Não espere pelo momento perfeito . Faça de cada momento a oportunidade perfeita . Não desista , você consegue\n"
     ]
    }
   ],
   "source": [
    "porter = PorterStemmer()\n",
    "porter_tokens = [porter.stem(t, to_lowercase=False) for t in word_tokens]\n",
    "print(join_string(porter_tokens))\n",
    "print(join_string(word_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "074bce10-945b-44cb-9973-0b7d297b5620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nã esper pel moment perfeit . fac de cad moment a oportun perfeit . nã desist , voc conseg\n",
      "Não espere pelo momento perfeito . Faça de cada momento a oportunidade perfeita . Não desista , você consegue\n"
     ]
    }
   ],
   "source": [
    "snow = SnowballStemmer(language=\"portuguese\")\n",
    "snow_tokens = [snow.stem(t) for t in word_tokens]\n",
    "print(join_string(snow_tokens))\n",
    "print(join_string(word_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a4cfb9c1-ea98-4a6f-9783-41f2d079da74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "não esp pelo momento perfeito . faç de cad momento a oportunidad perfeit . não desist , você consegu\n",
      "Não espere pelo momento perfeito . Faça de cada momento a oportunidade perfeita . Não desista , você consegue\n"
     ]
    }
   ],
   "source": [
    "lancaster = LancasterStemmer()\n",
    "lancaster_tokens = [lancaster.stem(t) for t in word_tokens]\n",
    "print(join_string(lancaster_tokens))\n",
    "print(join_string(word_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "db0626f4-bd78-495e-b208-08fb695b514f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Não esper pelo momento perfeito . Faça de cada momento a oportunidad perfeita . Não desista , você consegu\n",
      "nã esper pel moment perfeit . fac de cad moment a oportun perfeit . nã desist , voc conseg\n",
      "não esp pelo momento perfeito . faç de cad momento a oportunidad perfeit . não desist , você consegu\n"
     ]
    }
   ],
   "source": [
    "print(join_string(porter_tokens))\n",
    "print(join_string(snow_tokens))\n",
    "print(join_string(lancaster_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8039541-dd55-4d5d-aad6-50c288604765",
   "metadata": {},
   "source": [
    "### Lemmatização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5c0e6e6c-9296-4f89-80cb-0ada1737b266",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are still running , she can not run anymore .\n",
      "We are still running , she can not run anymore .\n"
     ]
    }
   ],
   "source": [
    "text_ = \"We are still running, she can not run anymore.\"\n",
    "text_ = word_tokenize(text_)\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "lemmas = [lemma.lemmatize(t) for t in text_]\n",
    "print(join_string(lemmas))\n",
    "print(join_string(text_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a4bdcf-1969-415d-8132-8d42c2a78b5a",
   "metadata": {},
   "source": [
    "### POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b17424d0-c514-45bc-9fca-9f0c67f34aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$: dollar\n",
      "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
      "'': closing quotation mark\n",
      "    ' ''\n",
      "(: opening parenthesis\n",
      "    ( [ {\n",
      "): closing parenthesis\n",
      "    ) ] }\n",
      ",: comma\n",
      "    ,\n",
      "--: dash\n",
      "    --\n",
      ".: sentence terminator\n",
      "    . ! ?\n",
      ":: colon or ellipsis\n",
      "    : ; ...\n",
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "CD: numeral, cardinal\n",
      "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
      "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
      "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "EX: existential there\n",
      "    there\n",
      "FW: foreign word\n",
      "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
      "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
      "    terram fiche oui corporis ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "JJR: adjective, comparative\n",
      "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
      "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
      "    cozier creamier crunchier cuter ...\n",
      "JJS: adjective, superlative\n",
      "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
      "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
      "    dearest deepest densest dinkiest ...\n",
      "LS: list item marker\n",
      "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
      "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
      "    two\n",
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "PDT: pre-determiner\n",
      "    all both half many quite such sure this\n",
      "POS: genitive marker\n",
      "    ' 's\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "PRP$: pronoun, possessive\n",
      "    her his mine my our ours their thy your\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "RBR: adverb, comparative\n",
      "    further gloomier grander graver greater grimmer harder harsher\n",
      "    healthier heavier higher however larger later leaner lengthier less-\n",
      "    perfectly lesser lonelier longer louder lower more ...\n",
      "RBS: adverb, superlative\n",
      "    best biggest bluntest earliest farthest first furthest hardest\n",
      "    heartiest highest largest least less most nearest second tightest worst\n",
      "RP: particle\n",
      "    aboard about across along apart around aside at away back before behind\n",
      "    by crop down ever fast for forth from go high i.e. in into just later\n",
      "    low more off on open out over per pie raising start teeth that through\n",
      "    under unto up up-pp upon whole with you\n",
      "SYM: symbol\n",
      "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
      "TO: \"to\" as preposition or infinitive marker\n",
      "    to\n",
      "UH: interjection\n",
      "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
      "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
      "    man baby diddle hush sonuvabitch ...\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "WDT: WH-determiner\n",
      "    that what whatever which whichever\n",
      "WP: WH-pronoun\n",
      "    that what whatever whatsoever which who whom whosoever\n",
      "WP$: WH-pronoun, possessive\n",
      "    whose\n",
      "WRB: Wh-adverb\n",
      "    how however whence whenever where whereby whereever wherein whereof why\n",
      "``: opening quotation mark\n",
      "    ` ``\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee43b2ec-9013-4ae4-bb35-e44669e22b34",
   "metadata": {},
   "source": [
    "##### Nível de token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4db160fa-5b65-4c78-ba4a-6b3f8a9f95e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Make', 'NNP'),\n",
       " ('every', 'DT'),\n",
       " ('moment', 'NN'),\n",
       " ('the', 'DT'),\n",
       " ('perfect', 'JJ'),\n",
       " ('opportunity', 'NN')]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Make every moment the perfect opportunity\"\n",
    "tokenizer = word_tokenize(text, language='english')\n",
    "\n",
    "pos = pos_tag(tokenizer, lang='eng')\n",
    "pos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491f7203-bdaa-459b-a9ed-3f4704db410f",
   "metadata": {},
   "source": [
    "##### Nível de sentença"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7e4b23f0-15e3-45a2-9267-4aa51e98dad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Make', 'every', 'moment', 'the', 'perfect', 'opportunity', '.'], ['Do', 'not', 'give', 'up', '.'], ['I', 'know', 'you', 'can', 'do', 'it']]\n"
     ]
    }
   ],
   "source": [
    "text2 = \"Make every moment the perfect opportunity. Do not give up. I know you can do it\"\n",
    "text_tks = sent_tokenize(text2, language='english')\n",
    "\n",
    "ntokens = []\n",
    "for sent in text_tks:\n",
    "    ntokens.append(word_tokenize(sent))\n",
    "print(ntokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5c33d9c2-1d1a-45e1-8a78-e0bdabc9b980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Make', 'NNP'),\n",
       "  ('every', 'DT'),\n",
       "  ('moment', 'NN'),\n",
       "  ('the', 'DT'),\n",
       "  ('perfect', 'JJ'),\n",
       "  ('opportunity', 'NN'),\n",
       "  ('.', '.')],\n",
       " [('Do', 'NNP'), ('not', 'RB'), ('give', 'VB'), ('up', 'RP'), ('.', '.')],\n",
       " [('I', 'PRP'),\n",
       "  ('know', 'VBP'),\n",
       "  ('you', 'PRP'),\n",
       "  ('can', 'MD'),\n",
       "  ('do', 'VB'),\n",
       "  ('it', 'PRP')]]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag_sents(ntokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4716e326-6fe5-4fe2-b1e6-3bf9a2b8eb77",
   "metadata": {},
   "source": [
    "### Entidades Nomeadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b1e2dc24-775b-400b-ac7e-da0de46ef617",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'svgling'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda1\\lib\\site-packages\\IPython\\core\\formatters.py:344\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    342\u001b[0m     method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n\u001b[0;32m    343\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 344\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda1\\lib\\site-packages\\nltk\\tree\\tree.py:782\u001b[0m, in \u001b[0;36mTree._repr_svg_\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_repr_svg_\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 782\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msvgling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m draw_tree\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m draw_tree(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m_repr_svg_()\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'svgling'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Tree('S', [Tree('GPE', [('Trump', 'NNP')]), ('lives', 'VBZ'), ('in', 'IN'), Tree('GPE', [('United', 'NNP'), ('States', 'NNPS')])])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ent = \"Trump lives in United States\"\n",
    "tokenize = word_tokenize(ent)\n",
    "tags = pos_tag(tokenize)\n",
    "\n",
    "entidades = nltk.ne_chunk(tags)\n",
    "entidades"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
